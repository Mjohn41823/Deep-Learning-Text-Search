{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week11 Assignment Final",
      "provenance": [],
      "authorship_tag": "ABX9TyPWlmhIFTeoahflFZGBMetL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mjohn41823/Deep-Learning-Text-Search/blob/main/Deep%20Learning%20Text%20Output_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM text generation"
      ],
      "metadata": {
        "id": "mDbjWJyk-RAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ol9XMC-G8q7f",
        "outputId": "119717b5-68b2-4100-9656-e761b3c72ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Length: 116578\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow import keras as tf\n",
        "\n",
        "path = 'Macbeth.txt'\n",
        "text = open(path).read().lower()\n",
        "print('Corpus Length:', len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing Sequences"
      ],
      "metadata": {
        "id": "7UVLPA5y-8wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 60\n",
        "\n",
        "step = 3\n",
        "\n",
        "sentences = []\n",
        "\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "\n",
        "print('Number of Sequences:', len(sentences))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Unique Characters\", len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "print('Vectorization')\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "      x[i,t,char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uEvIiEeL-7r6",
        "outputId": "578bac7c-5fe2-42d3-dbb4-af98b521083c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sequences: 38840\n",
            "Unique Characters 65\n",
            "Vectorization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single layer model for next character prediction"
      ],
      "metadata": {
        "id": "swgQxMQIAxcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n"
      ],
      "metadata": {
        "id": "XR7LkH0FAvaJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model configuration"
      ],
      "metadata": {
        "id": "RVsPZJaxBQXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.adam_v2\n",
        "model.compile(loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "5IW4Z9QyBUvd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to sample the next character give the model's prediction"
      ],
      "metadata": {
        "id": "wPKgYaGqHYEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "  "
      ],
      "metadata": {
        "id": "zlRYDtgPHXxh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generation Loop"
      ],
      "metadata": {
        "id": "AOJ7lwGaIIrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 20):\n",
        "     print('epoch', epoch)\n",
        "     model.fit(x, y, batch_size=128, epochs=1)\n",
        "     start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "     generated_text = text[start_index: start_index + maxlen]\n",
        "     print(\"-Generating with seed\")\n",
        "\n",
        "     for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "       print(\"temperature:\", temperature)\n",
        "       print(generated_text,\"\\nNext Loop\\n\", file=open('output.txt', 'a'))\n",
        "\n",
        "for i in range(400):\n",
        "  sampled = np.zeros((1, maxlen, len(chars)))\n",
        "  for t, char in enumerate(generated_text):\n",
        "    sampled[0, t, char_indices[char]] = 1\n",
        "  preds = model.predict(sampled, verbose=0) [0]\n",
        "  next_index = sample(preds, temperature)\n",
        "  next_char = chars[next_index]\n",
        "  generated_text += next_char\n",
        "  generated_text = generated_text[1:]\n",
        "  print(next_char,\"\\nNext Loop\\n\", file=open('output.txt', 'a'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XQUYRh9fIMEF",
        "outputId": "2ce79e92-de7d-4faa-e511-a8ed656675a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "304/304 [==============================] - 59s 193ms/step - loss: 0.9123\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 2\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8927\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 3\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8755\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 4\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8597\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 5\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8431\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 6\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8263\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 7\n",
            "304/304 [==============================] - 58s 192ms/step - loss: 0.8112\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 8\n",
            "304/304 [==============================] - 59s 192ms/step - loss: 0.7970\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 9\n",
            "304/304 [==============================] - 58s 191ms/step - loss: 0.7850\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 10\n",
            "304/304 [==============================] - 57s 187ms/step - loss: 0.7692\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 11\n",
            "304/304 [==============================] - 57s 186ms/step - loss: 0.7593\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 12\n",
            "304/304 [==============================] - 56s 185ms/step - loss: 0.7432\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 13\n",
            "304/304 [==============================] - 57s 186ms/step - loss: 0.7310\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 14\n",
            "304/304 [==============================] - 57s 186ms/step - loss: 0.7186\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 15\n",
            "304/304 [==============================] - 57s 186ms/step - loss: 0.7092\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 16\n",
            "304/304 [==============================] - 56s 186ms/step - loss: 0.6956\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 17\n",
            "304/304 [==============================] - 56s 185ms/step - loss: 0.6864\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 18\n",
            "304/304 [==============================] - 56s 185ms/step - loss: 0.6753\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n",
            "epoch 19\n",
            "304/304 [==============================] - 56s 185ms/step - loss: 0.6696\n",
            "-Generating with seed\n",
            "temperature: 0.2\n",
            "temperature: 0.5\n",
            "temperature: 1.0\n",
            "temperature: 1.2\n"
          ]
        }
      ]
    }
  ]
}